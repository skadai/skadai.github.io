<!DOCTYPE html><html domain="skadai.github.io" lang="en"><head><meta charset="utf-8"><meta content="width=device-width,initial-scale=1" name="viewport"><meta content="default-src 'self';object-src 'none';script-src 'self' 'sha256-Ky9qZOPnMhQV/s7Fdb9TYAOfU4KtWNqCZaFK8tSzXa0=';style-src 'unsafe-inline';img-src 'self' data:" http-equiv="Content-Security-Policy"><link href="/img/favicon/favicon-192x192.png?hash=2089033c93" rel="icon" type="image/png"><meta content="#f9c412" name="theme-color"><meta content="max-snippet:-1, max-image-preview: large, max-video-preview: -1" name="robots"><title>Triplet Loss and Online Triplet Mining (翻译)</title><meta content="Triplet Loss and Online Triplet Mining (翻译)" property="og:title"><meta content="This is a post on My Blog about CBIR" name="description"><meta content="This is a post on My Blog about CBIR" property="og:description"><meta content="summary_large_image" name="twitter:card"><meta content="@update_me" name="twitter:site"><meta content="@update_me" name="twitter:creator"><link href="https://skadai.github.io/posts/triplet_loss/" rel="canonical"><meta content="no-referrer-when-downgrade" name="referrer"><link href="/feed/feed.xml" rel="alternate" type="application/atom+xml" title="长安路"><link href="/" rel="preconnect" crossorigin=""><script async="" defer="" src="/js/min.js?hash=c372ec6a84"></script><script csp-hash="sha256-Ky9qZOPnMhQV/s7Fdb9TYAOfU4KtWNqCZaFK8tSzXa0=">if (/Mac OS X/.test(navigator.userAgent))document.documentElement.classList.add('apple')</script><style>:root{--primary: #f9c412;--primary-dark: #e7bf60;--main-width: calc(100vw - 3em)}main img{content-visibility:auto}article *{scroll-margin-top:50px}header nav{position:fixed;padding:.375em 1.5em;background:rgba(255,255,255,.9);font-weight:200;text-align:right}@media (min-width:47.5em){:root{--main-width: calc(47.5em - 3em)}}dialog,share-widget{position:fixed;opacity:.9}share-widget{right:20px;bottom:20px}share-widget div{width:30px;height:30px;background-image:url(/img/share.svg);background-repeat:no-repeat;background-position:center}.apple share-widget div{background-image:url(/img/share-apple.svg)}body,share-widget button{margin:0}share-widget button:active{transform:scale(1.2)}dialog{background-color:#8dff80;z-index:1000}header aside{font-style:italic}#nav{z-index:2;position:relative}#reading-progress,header nav{z-index:1;width:100vw;left:0;top:0}#reading-progress{background-color:var(--primary);position:absolute;bottom:0;transform:translate(-100vw,0);will-change:transform;pointer-events:none}#posts li{margin-bottom:.5em}@font-face{font-display:optional;font-family:"Inter UI";font-style:normal;font-weight:100;src:url(/fonts/Inter-Thin.woff2) format("woff2"),url(/fonts/Inter-Thin.woff) format("woff")}@font-face{font-display:optional;font-family:"Inter UI";font-style:italic;font-weight:100;src:url(/fonts/Inter-ThinItalic.woff2) format("woff2"),url(/fonts/Inter-ThinItalic.woff) format("woff")}@font-face{font-display:optional;font-family:"Inter UI";font-style:normal;font-weight:200;src:url(/fonts/Inter-ExtraLight.woff2) format("woff2"),url(/fonts/Inter-ExtraLight.woff) format("woff")}@font-face{font-display:optional;font-family:"Inter UI";font-style:italic;font-weight:200;src:url(/fonts/Inter-ExtraLightItalic.woff2) format("woff2"),url(/fonts/Inter-ExtraLightItalic.woff) format("woff")}@font-face{font-display:optional;font-family:"Inter UI";font-style:normal;font-weight:300;src:url(/fonts/Inter-Light.woff2) format("woff2"),url(/fonts/Inter-Light.woff) format("woff")}@font-face{font-display:optional;font-family:"Inter UI";font-style:italic;font-weight:300;src:url(/fonts/Inter-LightItalic.woff2) format("woff2"),url(/fonts/Inter-LightItalic.woff) format("woff")}@font-face{font-display:optional;font-family:"Inter UI";font-style:normal;font-weight:400;src:url(/fonts/Inter-Regular.woff2) format("woff2"),url(/fonts/Inter-Regular.woff) format("woff")}@font-face{font-display:optional;font-family:"Inter UI";font-style:italic;font-weight:400;src:url(/fonts/Inter-Italic.woff2) format("woff2"),url(/fonts/Inter-Italic.woff) format("woff")}@font-face{font-display:optional;font-family:"Inter UI";font-style:normal;font-weight:500;src:url(/fonts/Inter-Medium.woff2) format("woff2"),url(/fonts/Inter-Medium.woff) format("woff")}@font-face{font-display:optional;font-family:"Inter UI";font-style:italic;font-weight:500;src:url(/fonts/Inter-MediumItalic.woff2) format("woff2"),url(/fonts/Inter-MediumItalic.woff) format("woff")}@font-face{font-display:optional;font-family:"Inter UI";font-style:normal;font-weight:600;src:url(/fonts/Inter-SemiBold.woff2) format("woff2"),url(/fonts/Inter-SemiBold.woff) format("woff")}@font-face{font-display:optional;font-family:"Inter UI";font-style:italic;font-weight:600;src:url(/fonts/Inter-SemiBoldItalic.woff2) format("woff2"),url(/fonts/Inter-SemiBoldItalic.woff) format("woff")}@font-face{font-display:optional;font-family:"Inter UI";font-style:normal;font-weight:700;src:url(/fonts/Inter-Bold.woff2) format("woff2"),url(/fonts/Inter-Bold.woff) format("woff")}@font-face{font-display:optional;font-family:"Inter UI";font-style:italic;font-weight:700;src:url(/fonts/Inter-BoldItalic.woff2) format("woff2"),url(/fonts/Inter-BoldItalic.woff) format("woff")}@font-face{font-display:optional;font-family:"Inter UI";font-style:normal;font-weight:800;src:url(/fonts/Inter-ExtraBold.woff2) format("woff2"),url(/fonts/Inter-ExtraBold.woff) format("woff")}@font-face{font-display:optional;font-family:"Inter UI";font-style:italic;font-weight:800;src:url(/fonts/Inter-ExtraBoldItalic.woff2) format("woff2"),url(/fonts/Inter-ExtraBoldItalic.woff) format("woff")}@font-face{font-display:optional;font-family:"Inter UI";font-style:normal;font-weight:900;src:url(/fonts/Inter-Black.woff2) format("woff2"),url(/fonts/Inter-Black.woff) format("woff")}@font-face{font-display:optional;font-family:"Inter UI";font-style:italic;font-weight:900;src:url(/fonts/Inter-BlackItalic.woff2) format("woff2"),url(/fonts/Inter-BlackItalic.woff) format("woff")}@font-face{font-display:optional;font-family:"Inter UI var alt";font-weight:100 900;font-style:normal;font-named-instance:"Regular";src:url(/fonts/Inter-roman.var.woff2) format("woff2")}@font-face{font-display:optional;font-family:"Inter UI var alt";font-weight:100 900;font-style:italic;font-named-instance:"Italic";src:url(/fonts/Inter-italic.var.woff2) format("woff2")}button,html{line-height:1.15}html{-webkit-text-size-adjust:100%;font-family:Inter UI,sans-serif;--font-family: Inter UI, sans-serif}h1{font-size:3em;line-height:1.25;margin:.67em 0 .5em;font-size:2.074rem}a{background-color:transparent;color:#f9c412;text-decoration:none;color:var(--primary)}b,strong{font-weight:700}code{overflow-x:auto;border-radius:.3em;color:#e2777a;padding:0 .3em;font-family:Consolas,Monaco,Andale Mono,Ubuntu Mono,monospace;font-size:90%;background:#2d2d2d}small{font-size:80%;color:#ccc}img{border-style:none;max-width:100%;height:auto;margin:0 auto}button{font-family:inherit;font-size:100%;overflow:visible;text-transform:none}[type=button],button{-webkit-appearance:button}[type=button]::-moz-focus-inner,button::-moz-focus-inner{border-style:none;padding:0}[type=button]:-moz-focusring,button:-moz-focusring{outline:1px dotted ButtonText}h2{font-size:2.5em;line-height:1.2;margin-bottom:.6em}h3{font-size:2em;line-height:1.125;margin-bottom:.75em;font-size:1.4rem}h4{font-size:1.5em;margin-bottom:1em;line-height:1.5em}body,ol,p,ul{font-size:1em}form,ol,p,ul{margin-bottom:1.5em}h1,h2,h3,h4{line-height:2.4rem;margin-bottom:1.36rem}h2{font-size:1.728rem}h3,h4{line-height:1.6rem}h4{font-size:1.2rem}body,ol,p,ul{font-size:1rem;line-height:1.6}ol,p,ul{margin-bottom:1.36rem}@media (min-width:600px){h1{font-size:4.3978rem;line-height:4.4rem}h2{font-size:3.1097rem;line-height:3.52rem}h3{font-size:2.1989rem;line-height:2.64rem}h4{font-size:1.5554rem;line-height:1.76rem}body,ol,p,ul{font-size:1.1rem;line-height:1.6}h1,h2,h3,h4,ol,p,ul{margin-bottom:1.496rem}}@media (min-width:1200px){h1{font-size:6.0756rem;line-height:6.72rem}h2{font-size:4.05rem;line-height:4.8rem}h3{font-size:2.7rem;line-height:2.88rem}h4{font-size:1.8rem;line-height:1.92rem}body,ol,p,ul{font-size:1.2rem;line-height:1.6}h1,h2,h3,h4,ol,p,ul{margin-bottom:1.632rem}}h1,h2,h3,h4{font-family:var(--font-family)}form small,h2,h4{font-style:italic}a:hover{text-decoration:underline}@supports (font-variation-settings:normal){html{font-family:Inter UI var alt,sans-serif;--font-family: Inter UI var alt, sans-serif}}form{padding:1.5em 1.5em 0;border:.2rem solid #202020}button{border-radius:.3em;max-width:100%;background:#f2f2f2;color:#191919;cursor:pointer;padding:.75em 1.5em;text-align:center;margin:0 .75em 1.5em 0}button+label,label+*{page-break-before:always}button,label{display:inline-block}form>:not(fieldset){margin-right:.75em}button:hover{background:#d9d9d9;color:#000}button:not([disabled]){background:#f9c412;color:#181818;background:var(--primary)}button:not([disabled]):hover{background:#ba9005;color:#000;background:var(--primary-dark)}*{border:0;box-sizing:border-box}body,header nav a{font-family:var(--font-family)}body{background:#181818;color:#eee}header label{display:block}header{padding:4.5em 1.5em 3em;width:47.5em;margin:0 auto;text-align:center;max-width:100%;display:flex;align-items:center;flex-direction:column}header p,ol,ul{margin-top:0}header nav h1{float:left;font-size:inherit;line-height:inherit;margin:0;text-align:left}header nav a{font-weight:700;text-decoration:none;color:#181818;margin-left:1.5em}header nav a:first-of-type{margin-left:auto}header nav a:last-of-type{margin-right:1.5em}header nav label{color:#000;cursor:pointer;margin:0;font-style:normal;text-align:right}main{max-width:70rem;margin:0 auto;border-top:.5px solid #ccc}footer{background:rgba(0,0,0,.8);color:#fff;padding:3em;text-align:center}footer>*{margin:1.5em}footer nav a img{vertical-align:middle}footer nav,footer p{font-size:90%}article{max-width:100%;padding:1.5em;width:47.5em;margin:0 auto}li ol,li ul{margin-bottom:0}blockquote{border-left:1px solid #f9c412;padding:0 1.5em;margin:1.5em 0 1.5em 1.5em;border-left:1px solid var(--primary)}blockquote footer{background:0 0;display:block;color:#ccc;padding:.75em 0;font-size:90%;text-align:start}</style></head><body><header><nav><div id="nav"><h1><a href="/" title="Homepage">长安路</a></h1><a href="/tech/">技术</a> <a href="/history/">历史</a> <a href="/music/">音乐</a> <a href="/about/">关于</a></div><div id="reading-progress" aria-hidden="true"></div></nav><h2>Triplet Loss and Online Triplet Mining (翻译)</h2><aside>2 min read.</aside><dialog id="message"></dialog></header><main><article><blockquote><p>工作中碰到一个棘手的问题: 如何从海量商品中找到标准库中的那一个？<br>自己尝试过通过纯文本信息（标题, 商品简介, 图片ocr识别出文字）效果不佳, 决定换一个思路看看图像检索能不能提供一些有价值的信息。</p><p>本系列打算结合这个具体的问题, 系统总结下图像检索领域的基本概念</p></blockquote><p><strong>看到了 omoindrot的 <a href="https://omoindrot.github.io/triplet-loss">博客文章</a>对理解 triplet loss 的原理和实现都非常有帮助, 特此翻译</strong></p><h2 id="triplet-loss-and-triplet-mining">Triplet loss and triplet mining <a href="#triplet-loss-and-triplet-mining" class="direct-link">#</a></h2><h3 id="%E4%B8%BA%E4%BB%80%E4%B9%88%E4%B8%8D%E7%94%A8-softmax">为什么不用 softmax <a href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E4%B8%8D%E7%94%A8-softmax" class="direct-link">#</a></h3><p>Triplet loss (三元组损失) 是谷歌在经典论文 <a href="https://arxiv.org/abs/1503.03832">FaceNet: A Unified Embedding for Face Recognition and Clustering</a> 中提出的一种用来训练人脸 embedding 表征的损失函数。</p><p>和传统的分类问题(固定类别数目, 使用交叉熵损失函数)不同, 人脸识别, 商品检索往往是大量的不确定类别匹配问题, 这个时候更好的做法是判断两个item是不是属于同一个类别。</p><p>三元组损失就是为了上面的应用场景而出现的, 训练目标是来自同一个类别的样本距离要比来自不同类别的样本距离要近, 同一个列表样本形成一个类簇, 不同的类簇之间最好距离足够大。</p><h3 id="%E6%8D%9F%E5%A4%B1%E5%AE%9A%E4%B9%89">损失定义 <a href="#%E6%8D%9F%E5%A4%B1%E5%AE%9A%E4%B9%89" class="direct-link">#</a></h3><p><picture><source sizes="(max-width: 608px) 100vw, 608px" srcset="/img/remote/ZYdYNa-1920w.avif 1920w, /img/remote/ZYdYNa-1280w.avif 1280w, /img/remote/ZYdYNa-640w.avif 640w, /img/remote/ZYdYNa-320w.avif 320w" type="image/avif"><source sizes="(max-width: 608px) 100vw, 608px" srcset="/img/remote/ZYdYNa-1920w.webp 1920w, /img/remote/ZYdYNa-1280w.webp 1280w, /img/remote/ZYdYNa-640w.webp 640w, /img/remote/ZYdYNa-320w.webp 320w" type="image/webp"><source sizes="(max-width: 608px) 100vw, 608px" srcset="/img/remote/ZYdYNa-1920w.jpg 1920w, /img/remote/ZYdYNa-1280w.jpg 1280w, /img/remote/ZYdYNa-640w.jpg 640w, /img/remote/ZYdYNa-320w.jpg 320w" type="image/jpeg"><img alt="奥巴马和马克龙损失" decoding="async" height="763" loading="lazy" src="/img/remote/ZYdYNa-1920w.jpg" style="background-size:cover;background-image:url(&quot;data:image/svg+xml;charset=utf-8,%3Csvg xmlns='http%3A//www.w3.org/2000/svg' xmlns%3Axlink='http%3A//www.w3.org/1999/xlink' viewBox='0 0 1016 763'%3E%3Cfilter id='b' color-interpolation-filters='sRGB'%3E%3CfeGaussianBlur stdDeviation='.5'%3E%3C/feGaussianBlur%3E%3CfeComponentTransfer%3E%3CfeFuncA type='discrete' tableValues='1 1'%3E%3C/feFuncA%3E%3C/feComponentTransfer%3E%3C/filter%3E%3Cimage filter='url(%23b)' preserveAspectRatio='none' height='100%25' width='100%25' xlink%3Ahref='data%3Aimage/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAkAAAAHCAYAAADam2dgAAAACXBIWXMAAAsTAAALEwEAmpwYAAABDklEQVQY0wEDAfz+ALadiweqpptKvru4IMjExRHW19cQXFxcHE9PTxZaWloAmJiYAQBxZ2IgqYZy98Oupqf7/f/p9/b21MfHx1yioqIwy8vLAKenpwAAjntoDJR/coOAcm01yszMRsDAwEW5ubkQo6OjEQAAAAEAAAAAAMCmjh6tdlflvKWWlfT3+dDu7u29w8PDV3l5eR7d3d2M5OTklQDSs5cQpHxlmLCbj0vZ291m09LSYczMzCGampoP1NTUQ+bm5kUAQTstGaFsTrzIrp507/L1quzr6p29vb0+lZWVJvb29gDi4uIAAFRMORijb1jTwqmdfvr//4/19PSEw8PDQaenpyKOjo4A1dXVA5LFkO/9weVHAAAAAElFTkSuQmCC'%3E%3C/image%3E%3C/svg%3E&quot;)" width="1016"></picture></p><p>三元组损失的目标是在特征空间中满足</p><ol><li>相同标签的样本距离比较近</li><li>不同标签样本距离比较远</li></ol><p>然鹅, 我们不希望让每个标签的样本训练到特别小的空间内（这句话原文 <code>we don’t want to push the train embeddings of each label to collapse into very small clusters</code> 没有太理解, 作者专门解释了附在下面）</p><blockquote><p>When I mentioned that I meant that if you want to do classification or any task based on the embeddings, you only need the intra-cluster distance to be smaller than the inter-cluster distance. If you have that, then two examples from the same class will always have their embeddings closer than two examples from different classes.<br>Once this is done, you don't really need to push the clusters closer together. My intuition is that if you do so, it's a form of overfitting and you will lose in test accuracy.</p><p>In SphereFace they do something similar to softmax. Softmax is also going to push the predictions as close to 0 or to 1 as possible, which is to me a form of overfitting.</p></blockquote><p>啰嗦了半天, 损失具体是这样设计的</p><ul><li>anchor: 锚点样本(query 样本)</li><li>postive: 正样本(和锚点样本标签一样)</li><li>negative: 负样本(和锚点样本标签不一样)</li></ul><p><code>loss = max(d(a, p) - d(a, n) + margin, 0)</code></p><p>损失是锚点和正样本的距离减去负样本距离加上margin, 稍微解释一下<br>试想</p><ul><li>如果锚点和正样本距离很近, 和负样本距离很远, 也就是达到我们的理想embedding条件, 此时loss为0</li><li>如果锚点和正样本距离比较近, 和负样本距离比较远, 但是也没远多少, 此时因为有margin的存在, 这种情况loss会是一个 0-margin的值</li><li>锚点和正样本反而比较远, 和负样本比较近, 这个时候 loss 直接大于margin</li></ul><p>上面三种情况分别对应模型区分的简单(easy negative), 中等(semi-hard negative), 困难的(hard negative)情况, 因此loss是一个比一个大的, 下面的图比较生动</p><p><picture><source sizes="(max-width: 608px) 100vw, 608px" srcset="/img/remote/1R1iWm-1920w.avif 1920w, /img/remote/1R1iWm-1280w.avif 1280w, /img/remote/1R1iWm-640w.avif 640w, /img/remote/1R1iWm-320w.avif 320w" type="image/avif"><source sizes="(max-width: 608px) 100vw, 608px" srcset="/img/remote/1R1iWm-1920w.webp 1920w, /img/remote/1R1iWm-1280w.webp 1280w, /img/remote/1R1iWm-640w.webp 640w, /img/remote/1R1iWm-320w.webp 320w" type="image/webp"><source sizes="(max-width: 608px) 100vw, 608px" srcset="/img/remote/1R1iWm-1920w.jpg 1920w, /img/remote/1R1iWm-1280w.jpg 1280w, /img/remote/1R1iWm-640w.jpg 640w, /img/remote/1R1iWm-320w.jpg 320w" type="image/jpeg"><img alt="triplet illustration" decoding="async" height="725" loading="lazy" src="/img/remote/1R1iWm-1920w.jpg" style="background-size:cover;background-image:url(&quot;data:image/svg+xml;charset=utf-8,%3Csvg xmlns='http%3A//www.w3.org/2000/svg' xmlns%3Axlink='http%3A//www.w3.org/1999/xlink' viewBox='0 0 806 725'%3E%3Cfilter id='b' color-interpolation-filters='sRGB'%3E%3CfeGaussianBlur stdDeviation='.5'%3E%3C/feGaussianBlur%3E%3CfeComponentTransfer%3E%3CfeFuncA type='discrete' tableValues='1 1'%3E%3C/feFuncA%3E%3C/feComponentTransfer%3E%3C/filter%3E%3Cimage filter='url(%23b)' preserveAspectRatio='none' height='100%25' width='100%25' xlink%3Ahref='data%3Aimage/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAHCAIAAAC6O5sJAAAACXBIWXMAABibAAAYmwFJdYOUAAAAuklEQVQI1wGvAFD/AMbprcfjqOLRiuzFeubCedHJiLfUncfprQDF56vpy4H+rnHrjnHmjm70rWrVyIXD6K4A0dya+rdx+5B//o6C/42D+Zd67blvyuCjANnZlPquc/6OgPmSgv2QgPmQf/nAd8/dnQDO3p/6vHL7lH3+jIH9i4L6nnj1w3XI46YAxOiu39GL/bdw+6N1+6Z0/L1x19aTxOmuAMforMXnrNXYl+bOheTQh9Hbm8XorcforIGHeg3AtGlYAAAAAElFTkSuQmCC'%3E%3C/image%3E%3C/svg%3E&quot;)" width="806"></picture></p><h3 id="%E4%B8%89%E5%85%83%E7%BB%84%E7%AD%9B%E9%80%89(-triplet-mining)">三元组筛选( triplet mining) <a href="#%E4%B8%89%E5%85%83%E7%BB%84%E7%AD%9B%E9%80%89(-triplet-mining)" class="direct-link">#</a></h3><p>上面关于三元组损失的定义有了, 只是从原理上大体实现了训练目标, 那么具体实现上选择那些样本作为三元组做法是很多的, 三元组的筛选主要分为 offline and online 两类</p><h4 id="offline-mining-(%E4%B8%8D%E5%A5%BD%E7%94%A8)">offline mining (不好用) <a href="#offline-mining-(%E4%B8%8D%E5%A5%BD%E7%94%A8)" class="direct-link">#</a></h4><p>训练每个epoch之前, 把训练集全部embedding算出来, 找到 semi-hard 和 hard 进行训练, 计算量比较大</p><h4 id="online-mining-(%E5%A5%BD%E7%94%A8)">online mining (好用) <a href="#online-mining-(%E5%A5%BD%E7%94%A8)" class="direct-link">#</a></h4><p>在epoch每个batch训练的时候, 直接来, 不需要遍历全部的训练集, 就在一个小范围上面计算embedding并找到符合要求的</p><h3 id="%E5%9C%A8%E7%BA%BF%E7%AD%9B%E9%80%89%E7%AD%96%E7%95%A5">在线筛选策略 <a href="#%E5%9C%A8%E7%BA%BF%E7%AD%9B%E9%80%89%E7%AD%96%E7%95%A5" class="direct-link">#</a></h3><p>简而言之就是什么样的三元组需要进行训练, 在论文 <a href="https://arxiv.org/abs/1703.07737">In Defense of the Triplet Loss for Person Re-Identification.</a> 有详述</p><p>假设每个batch size 为 B = PK , 其中 P表示不同的标签, K表示每个标签挑选的样本数目, 我们只需要在这个小范围内进行挑选合适的三元组</p><h4 id="batch-all">batch all <a href="#batch-all" class="direct-link">#</a></h4><ul><li>选择所有合法的三元组(正样本和anchor同标签, 负样本和anchor标签不同), 刨除掉easy negative之后取平均loss</li><li>总共可能的三元组数目 PK(K-1)(PK-K), 依次表示, anchor可能数目, postive 可能数目, negative 可能数目</li></ul><h4 id="batch-hard">batch hard <a href="#batch-hard" class="direct-link">#</a></h4><ul><li>每一个anchor, 在当前 batch中 选择最困难的正样本(d(a,p) 最大) 和最困难的负样本( d(a, n)最小)形成三元组进行训练</li><li>总共可能的三元组就是 PK, 每个batch就只选择最困难的训练样本进行训练（论文声称这样比 batch all表现好, 好不好不知道, 但是训练时间短是真的）</li></ul><h4 id="%E5%90%8E%E7%BB%AD%E7%AB%A0%E8%8A%82(%E7%95%A5)">后续章节(略) <a href="#%E5%90%8E%E7%BB%AD%E7%AB%A0%E8%8A%82(%E7%95%A5)" class="direct-link">#</a></h4><p>下面是代码实现和示例项目, 直接看原博就好了。 尤其是作者关于 batch hard strategy 的实现, 可谓是赏心悦目, 值得好好学习</p><h4 id="%E6%84%9F%E6%83%B3">感想 <a href="#%E6%84%9F%E6%83%B3" class="direct-link">#</a></h4><p>triplet loss 的思想和之前看过的一个<a href="https://github.com/wvangansbeke/Unsupervised-Classification">文章</a>有一些相似, 都是关注样本内部相同标签和不同标签的差异性, 后者解决的是固定标签数目的无监督分类问题, 直接用图片的变化认为是一个正样本来做, anchor和正样本的距离尽可能小, 配合 self-labelling (把置信度高的直接当成标注数据) 让无监督分类的准确率直接达到了 85% 以上</p><p>对于商品匹配问题, 从类间差异和类内共性出发, 有没有可能挖掘一些弱监督的训练手段呢 ?</p><share-widget><button aria-label="Share" href="https://skadai.github.io/posts/triplet_loss/" on-click="share"><div></div></button></share-widget><script type="application/ld+json">{"@context":"https://schema.org","@type":"Article","headline":"Triplet Loss and Online Triplet Mining (翻译)","image":["https://skadai.github.io/img/remote/ZYdYNa-1920w.jpg","https://skadai.github.io/img/remote/1R1iWm-1920w.jpg"],"author":"sk","genre":"Insert a schema.org genre","publisher":{"@type":"Organization","name":"sk","logo":{"@type":"ImageObject","url":"/img/favicon/favicon-192x192.png?hash=2089033c93"}},"url":"https://skadai.github.io/posts/triplet_loss/","mainEntityOfPage":"https://skadai.github.io/posts/triplet_loss/","datePublished":"2021-08-17","dateModified":"2021-08-22","description":"工作中碰到一个棘手的问题: 如何从海量商品中找到标准库中的那一个？ 自己尝试过通过纯文本信息（标题, 商品简介, 图片ocr识别出文字）效果不佳, 决定换一个思路看看图像检索能不能提供一些有价值的信息。 本系列打算结合这个具体的问题, 系统总结下图像检索领域的基本概念 看到了..."}</script><p>Published <time datetime="2021-08-17">17 Aug 2021</time></p></article></main><footer><a href="/about/">sk</a></footer></body></html>